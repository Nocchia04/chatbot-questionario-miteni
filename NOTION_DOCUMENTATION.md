# Questionario Digitale PFAS Miteni - Documentazione Tecnica

---

## üìå Panoramica

Sistema di acquisizione dati strutturati mediante interfaccia conversazionale assistita da intelligenza artificiale, progettato per la raccolta di informazioni anagrafiche e questionari standardizzati relativi al caso PFAS Miteni.

**Versione**: 3.2.0  
**Ultimo Aggiornamento**: 29 Ottobre 2025  
**Status**: Beta Testing  
**Stack**: Next.js 15, TypeScript 5.x, OpenAI GPT-4o-mini, Google Sheets API

---

## üéØ Obiettivo del Progetto

Creare un'interfaccia conversazionale user-friendly per la raccolta dati nell'ambito dell'azione collettiva per il risarcimento dei danni da contaminazione PFAS causata dalla Miteni S.p.A. Il sistema sostituisce form statici tradizionali con un approccio dialogico assistito da AI, migliorando engagement e completamento del questionario.

---

## üèóÔ∏è Architettura del Sistema

### Paradigma Conversazionale

Il sistema adotta un approccio question-driven per la raccolta dati, sostituendo form statici con un'interfaccia dialogica che:
- Riduce l'abbandono della compilazione
- Migliora l'engagement dell'utente
- Gestisce input in linguaggio naturale
- Fornisce feedback immediato e validazione in tempo reale

### Stack Tecnologico

| Componente | Tecnologia | Versione |
|------------|-----------|----------|
| **Runtime** | Node.js | 18+ |
| **Framework** | Next.js (App Router) | 15.x |
| **Language** | TypeScript | 5.x |
| **AI Layer** | OpenAI GPT-4o-mini | Latest |
| **Storage (Dev)** | File System (JSON) | - |
| **Storage (Prod)** | In-Memory | - |
| **Data Export** | Google Sheets API | v4 |
| **Deployment** | Vercel Edge Network | Serverless |
| **Styling** | Tailwind CSS | 3.x |

---

## üé® Scelte Progettuali

### 1. Architettura a Macchina a Stati (FSM)

Il flusso conversazionale √® modellato come Finite State Machine in `lib/flow.ts`. Ogni stato rappresenta un punto di acquisizione dati con transizioni deterministiche basate su logica di validazione.

**Motivazione**: Garantisce coerenza del flusso, tracciabilit√† dello stato utente e resilienza a interruzioni (session resume).

**Stati principali**:
```
NOME ‚Üí COGNOME ‚Üí EMAIL ‚Üí TELEFONO ‚Üí MODALITA
                                    ‚Üì
                         [CHAT]     ‚Üì      [TELEFONO]
                            ‚Üì       ‚Üì         ‚Üì
                   SESSO ‚Üí ... ‚Üí R1-R17 ‚Üí FINE
```

### 2. AI-Assisted Input Processing

L'integrazione OpenAI non sostituisce la validazione strutturale ma funge da layer semantico per:
- **Disambiguazione intent utente**: distingue risposte da domande fuori contesto
- **Normalizzazione linguaggio naturale**: converte input libero in formato strutturato
- **Gestione conversazionale**: elabora input ambigui o incompleti con tono empatico

**Motivazione**: Riduce friction cognitivo per l'utente mantenendo qualit√† dati attraverso validazione programmatica a valle.

**Modello utilizzato**: GPT-4o-mini (scelta per velocit√† e costo-efficacia vs GPT-4)

### 3. Dual-Layer Validation

Ogni input attraversa due fasi di validazione:

1. **AI Interpretation** (`lib/aiConversation.ts`): Estrae intent e normalizza semantica
2. **Programmatic Validation** (`lib/validation/inputValidation.ts`): Verifica formato, range, coerenza

**Motivazione**: Combina flessibilit√† del linguaggio naturale con garanzie di integrit√† dati.

**Validatori implementati**:
- Email (regex pattern matching)
- Telefono italiano (normalizzazione +39)
- Provincia (mapping nome ‚Üí sigla)
- Data di nascita (formato gg/mm/aaaa, range 18-120 anni)
- Sesso (M/F)
- Modalit√† (CHAT/QUESTIONARIO flag strutturato)

### 4. Context Guardrails

Sistema di rilevamento off-topic (`lib/guardrails/contextGuardrail.ts`) basato su 70+ keyword matching e confidence scoring per limitare il dominio conversazionale allo scope PFAS/Miteni.

**Keywords monitorate**:
- PFAS, PFOA, PFOS, GenX, C6O4
- Miteni, Mitsubishi, ICIG
- Contaminazione, inquinamento, zone contaminate
- Risarcimento, azione collettiva, causa legale
- Salute, patologie, sintomi

**Motivazione**: Previene deriva conversazionale e garantisce focus sul task di data collection.

### 5. Knowledge Base Strutturata

Sistema di FAQ pre-approvate (`lib/knowledge/faqRisarcimento.ts`) con 11 risposte legalmente sicure che bypassano l'AI per garantire compliance comunicativa.

**FAQ implementate**:
1. Requisiti per richiedere risarcimento
2. Importi stimati
3. Calcolo personalizzato danno
4. Costi anticipati (zero)
5. Tempistiche (3-6 anni stimati)
6. Rischi (nulli per aderenti)
7. Documenti necessari
8. Funzionamento azione collettiva
9. Personalizzazione risarcimenti
10. Controparte (Mitsubishi)
11. Chi siamo (Finanziamento del Contenzioso S.p.A.)

**Motivazione**: Le risposte sono sottoposte a verifica legale prima dell'integrazione. L'intercettazione automatica garantisce che l'AI non possa deviare da formulazioni approvate.

### 6. Regole Comunicative Stringenti

Implementate regole critiche per sicurezza legale (`lib/knowledge/regoleComunicazione.ts`):

**SEMPRE**:
- Condizionale: "potrebbe", "potrebbe avere diritto"
- Tono formale (lei)
- Quantificatori sfumati: "molti", "alcuni", "numerosi"

**MAI**:
- Assoluti: "avrai", "otterrai", "certamente", "tutti"
- Garantire tempistiche precise
- Garantire risarcimento certo
- Collegare sentenza penale ‚Üí diritto automatico risarcimento
- Emoji

**Motivazione**: La controparte monitorer√† ogni comunicazione. Errori possono compromettere l'azione legale per migliaia di aderenti.

### 7. Session Persistence Strategy

**Locale**: File system JSON con auto-backup  
**Produzione (Vercel)**: In-memory con fallback graceful  
**Resume Logic**: Identificazione utente via email con merge intelligente di sessioni parziali

**Motivazione**: 
- File system locale per debugging e audit trail
- In-memory su serverless per performance e compliance con vincoli piattaforma
- Email-based resume per UX continuity senza richiedere autenticazione

### 8. Asynchronous Data Export

Google Sheets sync tramite `upsertSheetRow` con pattern fire-and-forget e retry logic.

**Motivazione**: Disaccoppia latenza I/O remoto dal response time percepito dall'utente. Il fallback garantisce che errori di export non bloccano progressione utente.

### 9. Rate Limiting & Circuit Breaker

**Rate Limiting**: IP-based con sliding window  
**Circuit Breaker**: Protezione chiamate OpenAI con exponential backoff

**Motivazione**: Protezione da abusi, contenimento costi API, graceful degradation sotto carico.

### 10. Serverless-First Design

Tutti i moduli critici (logger, storage, schedulers) implementano rilevamento ambiente Vercel con fallback appropriati.

**Motivazione**: Portabilit√† deployment locale ‚Üî serverless senza modifiche codice. Rispetta vincoli read-only filesystem di Lambda-like environments.

---

## üîÑ Flusso Dati End-to-End

```
User Input ‚Üí Frontend (React)
           ‚Üì
    POST /api/message
           ‚Üì
    Session Retrieval/Creation
           ‚Üì
    FAQ Handler (bypass AI se match)
           ‚Üì
    Context Guardrail Check
           ‚Üì
    AI Semantic Interpretation (GPT-4o-mini)
           ‚Üì
    Programmatic Validation
           ‚Üì
    State Transition (FSM)
           ‚Üì
    Google Sheets Upsert (async)
           ‚Üì
    Response + Next Question
```

---

## üìä Struttura Dati

### Conversation Context

```typescript
{
  sessionId: string,
  currentState: StateId,
  data: {
    [key: string]: {
      raw: string,           // Input originale utente
      normalized: string     // Valore validato e normalizzato
    }
  },
  history: Array<{
    from: "user" | "bot",
    text: string
  }>,
  metadata: {
    createdAt: ISO8601,
    lastModified: ISO8601,
    completed: boolean
  }
}
```

### Google Sheets Schema

| Colonna | Descrizione | Tipo |
|---------|-------------|------|
| NOME | Nome utente | String |
| COGNOME | Cognome utente | String |
| EMAIL | Email (chiave primaria) | String |
| TELEFONO | Telefono normalizzato (+39) | String |
| MODALIT√Ä | CHAT o QUESTIONARIO | Enum |
| SESSO | M o F | Enum |
| LUOGO DI NASCITA | Comune | String |
| PROVINCIA DI NASCITA | Sigla (VI, VR, PD, etc.) | String (2) |
| DATA DI NASCITA | gg/mm/aaaa | Date |
| R1-R17 | Risposte questionario | String |
| RIEPILOGO | Summary finale | Text |
| CONFERMA_FINALE | Consenso legale | String |

**Operazione**: Upsert basato su EMAIL come chiave primaria

---

## ‚ö° Performance

**Target Response Time**: < 2s (P95)

**Strategie di Ottimizzazione**:
- Modello AI lightweight (gpt-4o-mini vs gpt-4): 5x pi√π veloce
- Context window ridotto (ultimi 4 turni vs full history): -40% token usage
- Parallel processing non-blocking per Google Sheets
- Edge caching risorse statiche (Vercel CDN)
- FAQ pre-approvate bypassano AI: response time < 100ms

**Metriche osservate** (ambiente locale):
- Init sessione: ~150ms
- AI response: ~800ms (P50), ~1.5s (P95)
- FAQ response: ~50ms
- Sheet upsert: ~300ms (non-blocking)

---

## üîí Sicurezza & Privacy

- **API keys**: Gestite via environment variables (mai committate)
- **Rate limiting**: Prevenzione scraping e abuse
- **Input sanitization**: Pre-storage per prevenire injection
- **CORS policy**: Restrictive (solo domini autorizzati in produzione)
- **Google Service Account**: Least-privilege permissions (solo append/update sheet)
- **Session storage**: Ephemeral su serverless, encrypted in transit
- **Logging**: Structured logs senza PII sensibili in plaintext

---

## ‚ö†Ô∏è Limitazioni Architetturali

### Vercel Serverless

**Limitazioni**:
- No persistent file system ‚Üí sessioni in-memory (ephemeral)
- Cold start latency su low-traffic routes (~500ms)
- Timeout esecuzione funzione (10s default, 60s max)
- Limite payload request: 4.5MB

**Soluzione Consigliata per Produzione**: Migrazione a Redis/Vercel KV per session storage persistente e distribuzione edge.

### Google Sheets

**Limitazioni**:
- Rate limits API: 100 requests/100s per user
- Latenza scrittura: ~200-400ms per upsert
- Scalabilit√†: Performance degradation oltre 10k righe

**Mitigazione**: Retry logic con exponential backoff, async processing, future migration a PostgreSQL per dataset > 50k entries.

---

## ü§ñ Utilizzo di Large Language Models nel Processo di Sviluppo

### Contesto e Motivazioni

Il progetto √® stato sviluppato in regime di **beta testing accelerato** con timeline compresse per raggiungere rapidamente la fase di raccolta dati. L'utilizzo di Large Language Models (LLM) √® stato fondamentale per:

1. **Time-to-Market ridotto**: Sviluppo completato in 3 settimane vs 8-10 settimane stimate con approccio tradizionale
2. **Iterazioni rapide**: Implementazione e test di feature complesse in ore invece di giorni
3. **Qualit√† enterprise-grade**: Nonostante i tempi stretti, codice production-ready con best practices
4. **Documentazione estensiva**: Generazione automatica documentazione tecnica e FAQ

### Strumenti Utilizzati

#### Cursor AI + Claude 4.5 Sonnet (Sviluppo Codice)

**Utilizzo**:
- Generazione boilerplate e structure iniziale del progetto
- Implementazione pattern architetturali (FSM, validation pipeline, AI layer)
- Refactoring e ottimizzazione codice esistente
- Debug assistito e risoluzione errori complessi
- Integrazione API (OpenAI, Google Sheets)

**Vantaggi osservati**:
- Suggerimenti context-aware basati su codebase intero
- Correzione automatica errori TypeScript
- Generazione test cases
- Completamento intelligente funzioni complesse

**Esempio caso d'uso**: Implementazione sistema di session resume con email-based matching completata in 2 ore vs ~1 giorno stimato manualmente.

#### ChatGPT-5 (Arricchimento Documentazione)

**Utilizzo**:
- Generazione documentazione tecnica (README, API docs)
- Redazione FAQ con tone of voice professionale
- Traduzione documentazione IT/EN
- Creazione guide setup e deployment
- Revisione copy per compliance legale

**Prompt engineering specifico**:
```
Contesto: Documentazione tecnica per azione legale risarcimento PFAS
Tone: Professionale, formale, legalmente defensibile
Vincoli: SEMPRE condizionale, MAI assoluti, NO emoji
Output: Markdown formattato per GitHub/Notion
```

**Vantaggi osservati**:
- Consistency terminologica
- Adherenza a regole comunicative stringenti
- Generazione rapida varianti FAQ
- Quality assurance pre-approvazione legale

### Processo di Review e Quality Assurance

**IMPORTANTE**: Nonostante l'uso massiccio di LLM, **tutto il codice √® stato sottoposto a revisione umana** prima della pubblicazione.

**Pipeline di review**:

1. **Generazione LLM**: Claude/ChatGPT produce codice/documentazione
2. **Code Review manuale**: Verifica logica, sicurezza, performance
3. **Testing**: Unit tests, integration tests, manual QA
4. **Linting**: ESLint, TypeScript strict mode, Prettier
5. **Build verification**: npm run build deve passare senza errori/warning
6. **Legal review**: Documentazione rivista da consulenti legali (FAQ, regole comunicative)
7. **Deploy**: Solo dopo approvazione di tutti i check

**Metriche quality assurance**:
- 0 errori TypeScript in production
- 0 linting errors
- 100% delle FAQ approvate legalmente
- 100% codice LLM-generated sottoposto a review
- Test coverage: ~60% (target 80% per v1.0 stable)

### Aree di Contributo LLM

| Area | LLM Contribution | Human Review |
|------|-----------------|--------------|
| **Boilerplate code** | 90% | Architecture decisions |
| **Business logic** | 60% | Validation rules, edge cases |
| **AI prompts** | 80% | Tone refinement, legal compliance |
| **Validatori** | 70% | Regex patterns, edge cases |
| **UI components** | 75% | UX decisions, accessibility |
| **Documentazione** | 85% | Technical accuracy, legal review |
| **FAQ** | 60% | Legal approval, tone consistency |
| **CSS styling** | 80% | Brand colors, responsive design |
| **Error handling** | 65% | Edge cases, graceful degradation |

### Limitazioni e Disclaimer

**Riconosciamo che**:
- Il codice generato da LLM non √® infallibile
- La review umana √® essenziale per production-grade software
- Alcuni pattern potrebbero essere migliorati con esperienza diretta del team
- La documentazione richiede validazione continua

**Status Beta**: Il sistema √® attualmente in **beta testing**. Raccomandiamo:
- Monitoraggio continuo in produzione
- Feedback loop con utenti reali
- Iterazioni rapide su issue segnalate
- Migration path verso architettura enterprise se volumi crescono oltre aspettative

### Trasparenza e Etica

Crediamo nella **trasparenza sull'uso di AI nel processo di sviluppo**. Questa disclosure √® fornita per:
- Onest√† verso stakeholders e utenti finali
- Consapevolezza di punti di forza e limitazioni del sistema
- Best practice per progetti futuri
- Contributo al dibattito su AI-assisted development

**Commitment**: Continueremo a migliorare il sistema con combinazione di AI assistance e human expertise, mantenendo sempre la quality assurance come priorit√†.

---

## üîß Configurazione e Setup

### Prerequisiti

- Node.js 18+
- npm o yarn
- Account OpenAI (API key)
- Google Cloud Project con Sheets API abilitato
- Service Account Google con permessi su spreadsheet target

### Environment Variables

Creare file `.env.local` nella root del progetto:

```bash
# OpenAI API
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Google Sheets
GOOGLE_SHEETS_SPREADSHEET_ID=1ojzYokodglYKQSDxTJrPOhIMVwPhCVIdTw1mOqmN3xQ
GOOGLE_SHEETS_SHEET_NAME=Foglio1
GOOGLE_SHEETS_CREDENTIALS={"type":"service_account","project_id":"..."}
```

### Setup Locale

```bash
# Clone repository
git clone https://github.com/Nocchia04/chatbot-questionario-miteni.git
cd chatbot-questionario-miteni/mitenichat

# Install dependencies
npm install

# Setup environment
cp env.example .env.local
# Editare .env.local con le proprie credenziali

# Run development server
npm run dev

# Open browser
open http://localhost:3000
```

### Deploy Vercel

1. Push codice su GitHub
2. Connetti repository a Vercel
3. Configurare Environment Variables nel dashboard Vercel
4. Deploy automatico su ogni push a `main`

**URL Produzione**: https://chatbot-questionario-miteni.vercel.app

---

## üìà Monitoring e Maintenance

### Logs

**Locale**: Console output + file su `data/logs/app-YYYY-MM-DD.log`  
**Produzione**: Vercel Function Logs (dashboard)

**Structured logging format**:
```
[timestamp] [LEVEL] [sessionId] message {metadata}
```

### Health Check

```bash
GET /api/health

Response:
{
  "status": "healthy",
  "timestamp": "2025-10-29T10:00:00.000Z",
  "sessions": {...},
  "aiService": {
    "circuitBreaker": "CLOSED",
    "failures": 0
  },
  "rateLimit": {...},
  "uptime": 3600,
  "memory": {
    "used": 128,
    "total": 512
  }
}
```

### Backup

**Automatico** (solo locale):
- Sessioni: ogni 6h ‚Üí `data/backups/backup-*.json`
- Cleanup automatico: sessioni > 30gg

**Manuale**:
```bash
curl -X POST http://localhost:3000/api/admin/backup
```

---

## üêõ Troubleshooting

### Issue Comuni

**1. Errore "ENOENT: no such file or directory"**
- **Causa**: Ambiente Vercel (read-only filesystem)
- **Soluzione**: Verificare `IS_VERCEL` detection in `lib/storage/fileStorage.ts`

**2. Google Sheets "Unable to parse range"**
- **Causa**: Nome foglio non corrisponde a `GOOGLE_SHEETS_SHEET_NAME`
- **Soluzione**: Verificare nome tab nello spreadsheet

**3. AI responses lente (>5s)**
- **Causa**: Context window troppo grande o cold start
- **Soluzione**: Ridurre history slice in `aiConversation.ts` (gi√† ottimizzato a 4 turni)

**4. Session non riprende dopo reload**
- **Causa**: localStorage non persistente o sessionId non salvato
- **Soluzione**: Verificare che `localStorage.setItem(STORAGE_KEY, sessionId)` venga chiamato

---

## üìö Risorse Aggiuntive

- **Repository GitHub**: [chatbot-questionario-miteni](https://github.com/Nocchia04/chatbot-questionario-miteni)
- **Demo Live**: [https://chatbot-questionario-miteni.vercel.app](https://chatbot-questionario-miteni.vercel.app)
- **Caso Miteni**: Documentazione contesto in `lib/knowledge/casoMiteni.ts`
- **FAQ Legali**: Risposte approvate in `lib/knowledge/faqRisarcimento.ts`
- **Regole Comunicative**: Compliance rules in `lib/knowledge/regoleComunicazione.ts`

---

## üîÆ Roadmap Futura

### v1.0 (Stable Release)

- [ ] Migrazione session storage a Redis/Vercel KV
- [ ] Test coverage > 80%
- [ ] Performance monitoring (Sentry/Datadog)
- [ ] A/B testing framework
- [ ] Multi-language support (EN, DE)

### v1.1 (Enhancements)

- [ ] Voice input (Web Speech API)
- [ ] Document upload (carta identit√†, certificati)
- [ ] Real-time collaboration (multiple users per household)
- [ ] Analytics dashboard per admin
- [ ] Export PDF questionario compilato

### v2.0 (Enterprise)

- [ ] Migrazione database (PostgreSQL)
- [ ] Microservices architecture
- [ ] GraphQL API
- [ ] Mobile app (React Native)
- [ ] Advanced AI (fine-tuned model su casi legali italiani)

---

## üìÑ Licenza

Proprietario - Tutti i diritti riservati  
¬© 2025 Finanziamento del Contenzioso S.p.A.

Questo software √® sviluppato per uso interno nell'ambito dell'azione legale collettiva PFAS Miteni. Riproduzione, distribuzione o utilizzo non autorizzato √® vietato.

---

## üë• Contatti

**Finanziamento del Contenzioso S.p.A.**  
Sede: Brescia, Italia  
Web: www.libra.claims | www.prontodanno.it  
Progetto: RisarcimentoMiteni.it

**Technical Support**:  
Per issue tecnici, aprire ticket su GitHub repository

---

**Documento aggiornato**: 29 Ottobre 2025  
**Versione documentazione**: 1.1  
**Status**: Beta Testing

